{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fegZuoZfgOPe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761857349450,
     "user_tz": -60,
     "elapsed": 26547,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "e3f6bb88-ef2c-4397-f5a8-f72b139207ee"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting unsloth@ git+https://github.com/unsloth/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unsloth/unsloth.git)\n",
      "  Cloning https://github.com/unsloth/unsloth.git to /tmp/pip-install-mdhy8lhc/unsloth_f4ebbbf01ad040b2bf3214c3fcb1721a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unsloth/unsloth.git /tmp/pip-install-mdhy8lhc/unsloth_f4ebbbf01ad040b2bf3214c3fcb1721a\n",
      "  fatal: could not read Username for 'https://github.com': No such device or address\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u00d7\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/unsloth/unsloth.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/pip-install-mdhy8lhc/\u001b[0m\u001b[32munsloth_f4ebbbf01ad040b2bf3214c3fcb1721a\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m\u2570\u2500>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m\u00d7\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/unsloth/unsloth.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/pip-install-mdhy8lhc/\u001b[0m\u001b[32munsloth_f4ebbbf01ad040b2bf3214c3fcb1721a\u001b[0m did not run successfully.\n",
      "\u001b[31m\u2502\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m\u2570\u2500>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: peft>=0.8.0 in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, trl\n",
      "Successfully installed trl-0.24.0 xformers-0.0.32.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth[colab-new]@git+https://github.com/unsloth/unsloth.git\n",
    "!pip install --no-deps xformers \"peft>=0.8.0\" trl"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install unsloth \"peft>=0.8.0\" trl"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjrmTelfhea2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761857383682,
     "user_tz": -60,
     "elapsed": 34227,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "3e633c42-86cb-4db8-833b-ade71a9ba447"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.10.12-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/61.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: peft>=0.8.0 in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
      "Collecting unsloth_zoo>=2025.10.13 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.10.13-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.32.post2)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft>=0.8.0) (6.0.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft>=0.8.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.10.13->unsloth)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.10.13->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.13->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.10.13->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.13.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.3.1)\n",
      "Downloading unsloth-2025.10.12-py3-none-any.whl (348 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m348.7/348.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.10.13-py3-none-any.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m273.6/273.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, shtab, pyarrow, msgspec, tyro, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.24.0\n",
      "    Uninstalling trl-0.24.0:\n",
      "      Successfully uninstalled trl-0.24.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.19.0 pyarrow-22.0.0 shtab-1.7.2 torchao-0.14.1 trl-0.23.0 tyro-0.9.35 unsloth-2025.10.12 unsloth_zoo-2025.10.13\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Mistral"
   ],
   "metadata": {
    "id": "se-4cMeetzQS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# \ud83d\udca1 Modello BASE (Non Instruct)\n",
    "model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Caricamento del Modello e del Tokenizer in 4-bit (QLoRA)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Configurazione LoRA (Identica per l'efficienza)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHscfmrYhHvn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761655357340,
     "user_tz": -60,
     "elapsed": 58339,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "91080253-fe96-47d8-e611-b25debdecd86"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.10.10: Fast Mistral patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "# 1. Carica il tuo dataset\n",
    "# Sostituisci con il percorso del tuo file locale (.json, .csv) o un dataset HF\n",
    "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/dataset_final-gemini2-5.csv\", split = \"train\")\n",
    "\n",
    "# 2. Funzione di formattazione per la Generazione Continua (CLM)\n",
    "def formatting_function_clm(examples):\n",
    "    # La chiave 'text' \u00e8 il campo che il tokenizer affiner\u00e0\n",
    "    texts = []\n",
    "    instruction = \"Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole\"\n",
    "\n",
    "    # \ud83d\udcdd Esempio: assumi che i tuoi dati abbiano le colonne 'header' e 'content'\n",
    "    for clues_str, explanation_str, solution in zip(examples[\"clue\"], examples[\"explanation\"], examples['solution']):\n",
    "        # Convert the string representation of lists to actual lists\n",
    "        clues = ast.literal_eval(clues_str)\n",
    "        explanation = ast.literal_eval(explanation_str)\n",
    "\n",
    "        # \u27a1\ufe0f Unisci i campi in un singolo flusso di testo che vuoi che il modello apprenda.\n",
    "        # Usa un formato che separa logicaamente i documenti, ma senza prompt conversazionali.\n",
    "        text = f\"ISTRUZIONE: {instruction}\\n LE CINQUE PAROLE: {(', '.join(clues).upper())}\\n\\n RAGIONAMENTO: {(', '.join(explanation).upper())} \\nSOLUZIONE: {solution}\\n\\n<|endofturn|>\"\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Mappa la funzione al dataset\n",
    "dataset = dataset.map(formatting_function_clm, batched=True,)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c636023b5ef641588dcb2c1092f07152",
      "a1adb62a48694a27aa7e29c1e0f2bc3f",
      "6faa59081a2c4511bafe9afa235e6aeb",
      "9c36508b36104edaacc60f53457ae2e8",
      "2f8b400429f9420589d2b845c8f8e969",
      "7612a944d52c46b69e0c1294bb2d37db",
      "277e07913b1044e7a284217d5ab9f6eb",
      "8aaad1d2a12340628c9b4d7b81c826db",
      "e48c51a2fa1744e4a08c2546c943a11d",
      "42e8ce5feb984f73bccc298430281d81",
      "5835100f498248ad8f6c51f76fb14d65"
     ]
    },
    "id": "KtnZqBmkjnkD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761655357848,
     "user_tz": -60,
     "elapsed": 497,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "4d93716d-2d2e-46bd-bd6b-e905d2112266"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c636023b5ef641588dcb2c1092f07152"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzNCHQXe_3l0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761228100891,
     "user_tz": -120,
     "elapsed": 20,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "c39e2a2a-275c-40b1-8609-677acfe2ad94"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'title': \"La Ghigliottina - L'Eredit\u00c3\\xa0 19/11/2024'\",\n",
       " 'uploader': 'Rai',\n",
       " 'episode_day': '19/11/2024',\n",
       " 'url': 'https://www.youtube.com/watch?v=Yq5jddSisew',\n",
       " 'frame_namefile': '19-11-2024.jpg',\n",
       " 'words': 'GIORNO,PASSARE,LIBRO,GIOCHI,REALTA,23.125',\n",
       " 'result': '```json\\n{\\n  \"clue\": [\"GIORNO\", \"PASSARE\", \"LIBRO\", \"GIOCHI\", \"REALTA\"],\\n  \"explanation\": [\\n    \"fatti del giorno (quelli raccontati nei programmi di attualit\u00e0)\",\\n    \"passare alle parole ai fatti (prendere una decisione e metterla in pratica)\",\\n    \"libro dei fatti (riferimento al libro che racconta determinati avvenimenti, come il Guinness World Record)\",\\n    \"a giochi fatti (nel momento in cui si \u00e8 scritto)\",\\n    \"realt\u00e0 dei fatti (perch\u00e9 \u00e8 una persona concreta che guarda alla realt\u00e0 dei fatti)\"\\n  ],\\n  \"solution\": \"FATTI\"\\n}\\n```',\n",
       " 'clue': \"['GIORNO', 'PASSARE', 'LIBRO', 'GIOCHI', 'REALTA']\",\n",
       " 'explanation': \"['fatti del giorno (quelli raccontati nei programmi di attualit\u00e0)', 'passare alle parole ai fatti (prendere una decisione e metterla in pratica)', 'libro dei fatti (riferimento al libro che racconta determinati avvenimenti, come il Guinness World Record)', 'a giochi fatti (nel momento in cui si \u00e8 scritto)', 'realt\u00e0 dei fatti (perch\u00e9 \u00e8 una persona concreta che guarda alla realt\u00e0 dei fatti)']\",\n",
       " 'solution': 'FATTI',\n",
       " 'text': 'ISTRUZIONE: Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole e la spiegazione\\n LE CINQUE PAROLE: GIORNO, PASSARE, LIBRO, GIOCHI, REALTA\\n\\nSOLUZIONE: FATTI\\n\\n<|endofturn|>'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # \u2b05\ufe0f Usa il campo di testo continuo\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_workers = 4,\n",
    "    packing = False,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,    # Basso per Colab T4\n",
    "        gradient_accumulation_steps = 4,  # Simula un batch size di 8\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 6,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# AVVIA IL FINE-TUNING\n",
    "trainer_stats = trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c1db599e66fd412295ba412eed3d5331",
      "d5dec2fdb310468cad715e5dcf325072",
      "8ff5071fb80f485c9d1962df70d6f6d6",
      "90c8774c78d14065b646b157fed02372",
      "3a1f3eff859e4142a0872641b35e1d74",
      "a3ba229c6cd044859c709caaa64e6859",
      "665f2b88d8314ed69259a130f7f2f719",
      "bcbfb9024a9e4fde8b5e6a1b87f30758",
      "ed24b3f1add6497eae9907cc11c41ade",
      "4df5a3dd388f49fa9ea751d096804664",
      "2e231500ed93469086a45ab0dcb042e4"
     ]
    },
    "id": "ITywwpaVnu69",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761658217282,
     "user_tz": -60,
     "elapsed": 2859429,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "72a9fb69-2726-4e73-8ba8-bcebdcc5e686"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/454 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1db599e66fd412295ba412eed3d5331"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 454 | Num Epochs = 6 | Total steps = 342\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 7,283,675,136 (0.58% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 47:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.821400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.778200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.769700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.580700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Salva gli adapter LoRA\n",
    "# model.save_pretrained_merged(\"mistral-base-finetuned-ghigliottinai2\", tokenizer, save_method = \"lora\")\n",
    "# 3. Usa il metodo nativo PEFT 'save_pretrained' sul modello\n",
    "# Questo salva automaticamente solo i pesi LoRA aggiunti.\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch\"\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 4. Salva il Tokenizer separatamente (\u00e8 comunque piccolo)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qw0rXmWlpK5k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761658219730,
     "user_tz": -60,
     "elapsed": 2443,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "094dea49-c9a7-4e17-d428-b89a5d7aaa55"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/chat_template.jinja',\n",
       " '/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/tokenizer.model',\n",
       " '/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/added_tokens.json',\n",
       " '/content/drive/MyDrive/mistral_instruct_adapter/with-explanation-6epoch/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_prompt(clues):\n",
    "  instruction = \"Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole\"\n",
    "  text = f\"{instruction}\\n LE CINQUE PAROLE: {(\", \".join(clues).upper())}\\n\"\n",
    "  return text"
   ],
   "metadata": {
    "id": "bQrHgNpuo4KG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761860634939,
     "user_tz": -60,
     "elapsed": 8,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "# \u26a0\ufe0f DEFINISCI LA TUA SEQUENZA DI STOP ESATTA\n",
    "STOP_SEQUENCE = \"<|endofturn|>\"\n",
    "stop_token_ids = tokenizer.encode(STOP_SEQUENCE, add_special_tokens=False)\n",
    "stop_token_ids = torch.tensor(stop_token_ids, device=model.device)\n",
    "stop_len = len(stop_token_ids)\n",
    "\n",
    "# --- CLASSE AGGIORNATA ---\n",
    "class RobustStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids, start_idx):\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "        self.stop_len = len(stop_token_ids)\n",
    "        self.start_idx = start_idx # Indice da cui iniziare a controllare\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Controlla solo gli ultimi token generati\n",
    "        current_sequence = input_ids[0]\n",
    "\n",
    "        # Inizia a controllare solo dopo che il modello ha generato almeno la lunghezza del token di stop\n",
    "        if current_sequence.shape[-1] < self.stop_len:\n",
    "            return False\n",
    "\n",
    "        # Controlla se la sequenza di stop si trova nell'ultima porzione dell'output\n",
    "        # Controlliamo in una finestra che \u00e8 grande quanto la lunghezza del token di stop\n",
    "\n",
    "        # Controlla la corrispondenza esatta con gli ultimi token\n",
    "        if torch.equal(current_sequence[-self.stop_len:], self.stop_token_ids):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "# 3. Crea la lista dei criteri di stop\n",
    "stopping_criteria = StoppingCriteriaList([RobustStoppingCriteria(stop_token_ids, inputs['input_ids'].shape[1])])"
   ],
   "metadata": {
    "id": "AzP3oU_HMZdj",
    "executionInfo": {
     "status": "error",
     "timestamp": 1761652904499,
     "user_tz": -60,
     "elapsed": 26,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "outputId": "4123e691-b3c3-4a2d-e271-5add0069572a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2201880835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# 3. Crea la lista dei criteri di stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mstopping_criteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRobustStoppingCriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# Per ricaricare il modello per l'inferenza (in un nuovo notebook)\n",
    "# Ricarica il modello base (in 4-bit)\n",
    "# base_model, _ = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_name,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     load_in_4bit = True,\n",
    "# )\n",
    "\n",
    "# # Carica gli adapter LoRA e uniscili al modello base\n",
    "# peft_model = PeftModel.from_pretrained(base_model, \"mistral-base-finetuned-ghigliottinai\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Prepara il modello per l'inferenza\n",
    "\n",
    "prompt = create_prompt(clues=['maestro', 'fuori', 'cicale', 'rispondere', 'proteste'])\n",
    "print(prompt)\n",
    "# Tokenizzazione\n",
    "inputs = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generazione del testo (completamento)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer = streamer,\n",
    "    max_new_tokens = 256,\n",
    "    use_cache = True,\n",
    "    #stopping_criteria = stopping_criteria\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4YOSBBgpYK_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761658232860,
     "user_tz": -60,
     "elapsed": 13115,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "f05fe3f8-a526-40fd-f94c-4d1672042185"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pensa passo dopo passo usando la chain of thoughts \n",
      "ISTRUZIONE: Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole\n",
      " LE CINQUE PAROLE: MAESTRO, FUORI, CICALE, RISPONDERE, PROTESTE\n",
      "\n",
      "\n",
      "RAGIONAMENTO: MAESTRO DEL VIOLINO (RIFERIMENTO A PINOCCHIO), FUORI FORMA (RIFERIMENTO A PINOCCHIO E ALLE PROTESTE), CICALE E IL VIOLINO (RIFERIMENTO A 'CICALE DEL VIOLINO'), RISPONDERE COL VIOLINO (RIFERIMENTO A SARAH McLACHLAN E AL SUO TALENTO), VIOLINO DELLE PROTESTE (RIFERIMENTO AI GIORNI DI CORPO LIBERO E AI PROTESTATORI) \n",
      "SOLUZIONE: VIOLINO\n",
      "\n",
      "<|endofturn|></s>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQIAa_7IJhxQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761857402753,
     "user_tz": -60,
     "elapsed": 19051,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "474ebac8-57c5-4b86-8675-af55eab3322f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#caricamento da checkpoint precedente\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# \u26a0\ufe0f ASSICURATI DI MONTARE GOOGLE DRIVE QUI se hai salvato su Drive\n",
    "# from google.colab import drive; drive.mount('/content/drive')\n",
    "\n",
    "# Ricarica i parametri base (devono essere IDENTICI al training)\n",
    "max_seq_length = 2048\n",
    "model_name = \"unsloth/mistral-7b-bnb-4bit\" # Usa lo stesso modello base!\n",
    "\n",
    "# 1. Carica il modello base in 4-bit (questa \u00e8 l'operazione da 15 GB)\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# \u26a0\ufe0f CAMBIA 'OUTPUT_DIR' con il PERCORSO ESATTO dove hai salvato l'adapter\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/mistral_base_adapter\"\n",
    "\n",
    "# 2. Carica l'adapter LoRA (solo i ~100 MB) e uniscilo al modello base\n",
    "# PeftModel prende il modello base (base_model) e applica i pesi da OUTPUT_DIR\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "\n",
    "# 3. Prepara per l'inferenza (passaggio Unsloth per ottimizzare la generazione)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"\u2705 Modello base e Adapter LoRA caricati e uniti in memoria. Pronto per l'inferenza.\")"
   ],
   "metadata": {
    "id": "hQQniTYWDuOo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## let's try with Gemma fine-tuned on reasoning\n"
   ],
   "metadata": {
    "id": "-gECsTdtEEoA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# 1. Carica il modello Gemma con QLoRA/4bit\n",
    "# Usiamo Gemma-3 4B IT per essere veloci e accessibili in ambienti con VRAM limitata\n",
    "max_seq_length = 2048 # Imposta la lunghezza massima della sequenza\n",
    "dtype = None # Seleziona automaticamente bfloat16 o float16\n",
    "load_in_4bit = True # Usa QLoRA per l'efficienza\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/gemma-3-4b-it\", # o \"gemma-7b-it\" se hai pi\u00f9 VRAM\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. Configura gli adattatori LoRA\n",
    "# Allena solo i moduli di attenzione (standard LoRA)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Rango LoRA. 16 \u00e8 un buon valore di partenza.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Ottimizzazione per VRAM\n",
    "    random_state = 42,\n",
    "    use_flash_attention_2 = True, # Velocizza l'addestramento\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "# 3. Carica e formatta il Dataset GSM8K per CoT\n",
    "\n",
    "# Carica la traccia di training di GSM8K\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\", split = \"train\")\n",
    "\n",
    "# Formattazione del prompt per la CoT.\n",
    "# Gemma IT usa <start_of_turn>user / <start_of_turn>model\n",
    "def formatting_prompts_func(examples):\n",
    "    # La chiave \u00e8 inserire la Chain-of-Thought (CoT) prima della risposta finale\n",
    "    # La colonna 'answer' in GSM8K contiene la CoT e la risposta finale\n",
    "\n",
    "    formatted_texts = []\n",
    "\n",
    "    # GSM8K ha la colonna 'question' e 'answer' (che include il ragionamento)\n",
    "    for question, answer in zip(examples['question'], examples['answer']):\n",
    "        # Formato Gemma Instruction-Tuned (IT)\n",
    "        text = f\"\"\"<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{answer}<end_of_turn>\"\"\"\n",
    "        formatted_texts.append(text)\n",
    "\n",
    "    return { \"text\" : formatted_texts }\n",
    "\n",
    "# Applica la formattazione\n",
    "gsm8k_dataset = gsm8k_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 4. Avvia l'Addestramento\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = gsm8k_dataset,\n",
    "    dataset_text_field = \"text\", # Colonna di testo che contiene l'input e l'output CoT\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_workers = 4,\n",
    "    packing = True, # Pacchettizza pi\u00f9 esempi per un uso efficiente della GPU\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1, # Di solito 2-3 epoch sono sufficienti per CoT\n",
    "        learning_rate = 5e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        report_to=\"none\",\n",
    "        output_dir = \"gemma_4b_cot_finetune\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Salva il modello LoRA adattato\n",
    "#model.save_pretrained_merged(\"gemma_cot_merged_model\", tokenizer, save_method = \"json\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87f81715aab6480c8430ef9335152cec",
      "efa540e088b940d6a82dd0d8de3234c1",
      "0a8719a13d194524becb165e130bcf45",
      "0813bd9a1ad9471185c1128217144095",
      "8d30b7a19396472bb96c5fcd962c93f3",
      "4dfe964ed0294ff198c641866cc21f5b",
      "e1344516089c434d895e88bedfe031d5",
      "31c880a4a10748a885c7e69003ecc239",
      "e77986e8bdac44b98b2c10ac780a1c14",
      "a997e791b99b4dd9b420934fa1183849",
      "938fd82ea9c3463aab867af38e0637fa",
      "e2f14db8cdc742539b37696b21430c48",
      "976da1b5eaeb41ad804e05e9e73521b2",
      "b4894a4bf67345afabc20531111dd5cc",
      "7b96d7021e2e4911a65df17cc08ddce4",
      "9c05767e1ebd4549842201dd39c4ea15",
      "04184d48bd504d729ba3a7b05a6a779e",
      "055863fc2fef43ca9ad5b1a428ee122f",
      "03145ed32a724b9b9c2c3f0cffa9abe9",
      "523398fc4da14edb809a8d71f24190f2",
      "91ca4d947007454d800a11adc647eeb0",
      "59d6ca35d6e74399953816ff1cf7251d",
      "2abb0a73db494366a3a34679dc45b16c",
      "ffce56cf0a7649759490bc0f31172c76",
      "b438e463e82343caac5a943df4d4e87e",
      "715f9f4ba8f6499dbf5b78d474042af5",
      "92f5979a12f0410b941dcd86891c9912",
      "d60bd43fed31402e84e69ba09b0385b2",
      "549a2cc6f0a84bd38d2c77af2c4207e9",
      "b8e207e807534ee3ade3a8866ab268f8",
      "a1d23da46f66474f9f240a949c5d2daf",
      "ac8fa6ba997c49ed9808f051db82c5d4",
      "08996f2455be48848dc101c1eab26a61"
     ]
    },
    "id": "X4sXZfN8EJCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761743423004,
     "user_tz": -60,
     "elapsed": 7238471,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "6ef1f7bf-c31a-4551-adda-d95b3f497c30"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO 10-29 11:10:25 [__init__.py:216] Automatically detected platform cuda.\n",
      "ERROR 10-29 11:10:27 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.11: Fast Gemma3 patching. Transformers: 4.57.1. vLLM: 0.11.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f81715aab6480c8430ef9335152cec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2f14db8cdc742539b37696b21430c48"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2abb0a73db494366a3a34679dc45b16c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 935\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 32,788,480 of 4,332,867,952 (0.76% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 1:57:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=935, training_loss=0.15180019796850847, metrics={'train_runtime': 7077.8352, 'train_samples_per_second': 1.056, 'train_steps_per_second': 0.132, 'total_flos': 3.770352105702e+16, 'train_loss': 0.15180019796850847, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#facciamo il finetuning a partire dal dataset creato da me con la chain of thoughts della ghigliottina\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# 1. Carica il modello Gemma con QLoRA/4bit\n",
    "# Usiamo Gemma-3 4B IT per essere veloci e accessibili in ambienti con VRAM limitata\n",
    "max_seq_length = 2048 # Imposta la lunghezza massima della sequenza\n",
    "dtype = None # Seleziona automaticamente bfloat16 o float16\n",
    "load_in_4bit = True # Usa QLoRA per l'efficienza\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/gemma-3-4b-it\", # Changed from Gemma 3 4B to Gemma 2 9B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "instruction = 'Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole \\nLE CINQUE PAROLE: '\n",
    "\n",
    "# 2. Configura gli adattatori LoRA\n",
    "# Allena solo i moduli di attenzione (standard LoRA)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Rango LoRA. 16 \u00e8 un buon valore di partenza.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Ottimizzazione per VRAM\n",
    "    random_state = 42,\n",
    "    use_flash_attention_2 = True, # Velocizza l'addestramento\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "# Carica la traccia di training\n",
    "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/ghigliottinAI/dataset_cot4.csv\", split = \"train\")\n",
    "\n",
    "# Formattazione del prompt per la CoT.\n",
    "# Gemma IT usa <start_of_turn>user / <start_of_turn>model\n",
    "def formatting_prompts_func(examples):\n",
    "    # La chiave \u00e8 inserire la Chain-of-Thought (CoT) prima della risposta finale\n",
    "    # La colonna 'answer' in GSM8K contiene la CoT e la risposta finale\n",
    "\n",
    "    formatted_texts = []\n",
    "\n",
    "    # GSM8K ha la colonna 'question' e 'answer' (che include il ragionamento)\n",
    "    for clues, cot in zip(examples['clue'], examples['cot']):\n",
    "        # Formato Gemma Instruction-Tuned (IT)\n",
    "        text = f\"\"\"<start_of_turn>user\n",
    "{instruction}{clues}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{cot}<end_of_turn><eos>\"\"\"\n",
    "        formatted_texts.append(text)\n",
    "    return { \"text\" : formatted_texts }\n",
    "\n",
    "# Applica la formattazione\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dividi il dataset\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "# Poi usa train_dataset e eval_dataset nel trainer\n",
    "\n",
    "\n",
    "# 4. Avvia l'Addestramento\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_workers = 4,\n",
    "    packing = False,  # Disabilitato per preservare la struttura del reasoning\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,  # Ridotto\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,  # Ridotto drasticamente\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 5,  # Pi\u00f9 frequente per monitorare meglio\n",
    "        eval_strategy = \"steps\",  # Aggiungi evaluation\n",
    "        eval_steps = 20,  # Valuta ogni 20 steps\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 20,\n",
    "        load_best_model_at_end = True,  # Carica il miglior modello\n",
    "        metric_for_best_model = \"loss\",\n",
    "        save_total_limit = 2,  # Mantieni solo i 2 migliori checkpoint\n",
    "        report_to=\"none\",\n",
    "        output_dir = \"gemma_4b_cot_finetune\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,  # Aggiungi regolarizzazione\n",
    "        seed = 42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Salva il modello LoRA adattato\n",
    "#model.save_pretrained_merged(\"gemma_cot_merged_model\", tokenizer, save_method = \"json\")"
   ],
   "metadata": {
    "id": "lp6t-6RLmruK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aa0509efaada4931b7848ee2d4e6612a",
      "371471ab21cb40068cb6f8679a1f4b6f",
      "0019914ca6df492e88faf42fdc7a0c86",
      "49b6a56473934c60893e5c48cf390bc7",
      "3df9247c7cf64a52b421ef43de5284fb",
      "91d1967272fd41148e93e4499c3ca64e",
      "beead69852a2445587447817e3efb008",
      "5bc3835836184ea5a40ab766af44debf",
      "4d2a94138ae84c958bd9a22ba2d700b8",
      "4aee493aa02b4e0bbf2ffff6c6841418",
      "c33c1162aef7425c9ef72b758397d61b",
      "99a38f753bee444d8058d17b223fbaa4",
      "c5b00d8ac2554436910693898dbb5764",
      "6eb7722718494f24a6c9a75e5a460083",
      "7973cbf34a18496cbc2432dd4773c520",
      "fea2a49b9844403e93095c9197e465e4",
      "03554750e61d4061a187ee849324af4b",
      "785a538879de44e5bf9a2ae608efe421",
      "beba15b55afc44c0b5b5feb391293a18",
      "e070b40573c141f883da13a0854b7397",
      "2108a1dd1506458f9459b30e67ea7c82",
      "113a189a4e824a36b638371680fed197",
      "6fbda632db4e42e1a8ff489ea1793243",
      "77ca3b3d5ff84473bc018f20dc86021f",
      "a71eab3d1bed462b96b6c65382dd446d",
      "d668a8ca5f00418085a2081be294a8e7",
      "dc8cdfcbe36b47e8b086ba536f89a704",
      "ae0810372e9f4a39a71f5f40ec36185a",
      "dbd96c52eeb549b3ad73ea00c4099f91",
      "d45e6a54e52f4c29926e73504de5242c",
      "32dc265630e444438bf07c56402e148b",
      "7befc4935cf744fca237b05d65828737",
      "ff450eace46b49119acf10495cdeedee",
      "b3abdcf94381422d9dc84e005730cdd4",
      "4da5efe9deb54e9aa7d1fd910633aafa",
      "55be9eee199541ed8c47990bb7152a08",
      "489d38b4a983435c8cc6c507567b18e2",
      "472c2243045a4a73a17cc410e8d3efa1",
      "6021d91c0c944e0aba0f43bac5cd529c",
      "ac95e33cf8d34c528ab85b767112d8b9",
      "14e3dea7d9c1499998ac160bdfedc44e",
      "ce1310b4e3eb4d188a92424161a7d3f2",
      "4496a272f0774d349043e50878d1ab58",
      "d61b994d0f6e407893f1543a84de06c0",
      "558924138f2c44eebea6323ee008772d",
      "18914c1a5cae4f7a99465f0fc2d1da62",
      "c8c9e61864904cef82e8a19ac2f5bf0a",
      "c7030747e897484a8e087b1e4637e858",
      "20acfc19da4c4c4db8ad4c121fac8af3",
      "d8c9c3250215493a892feccb868de79b",
      "31f49e881e824e13811ca9fd87e6aeb2",
      "6feecdc038bd45f898fe7f1e013b2a17",
      "7bf3734fc8904de4b1d7d9117c171450",
      "233589420e324428aa097a02b9a3af33",
      "170e2bee426f4e4cb240d4a6ac7ff914",
      "fcffd9a5313043e2ae9c945c6fb2b798",
      "5b306726a46849f5a404437bf6d57b61",
      "7f9670fdbac244c2b5ae945d9bf008f1",
      "de59c1510a9e4c6e8a24566dee5bae24",
      "fee1d912ea4b4358a5d2fdf2bbf53b56",
      "15aa321a48c4472e861b637b260a0118",
      "51957b4152ef423990febcfdf962aec9",
      "80ea365b24d24dd6b8ae6c211218bb43",
      "8111ee48bde5420b810a76ad9ec6867b",
      "45ba19452e6e4ec6804d9e8d95eb2398",
      "ac147bec3de4453ca499e306b036e6a1",
      "d4b7ee95063744f6b0ca294de6fc0e31",
      "978258697b724b978886be177e38b1f9",
      "7ed4717b830947e587d38ad2997a6539",
      "a48b42e010a24906891696f1d82f861a",
      "bce3c91b146240d49c6815343dfbb61f",
      "abd777d745e84a7db4fd78019922e693",
      "335bd712e43c48dbbdf9c06882ad185e",
      "8c6a1d6826914e26844f01854a21f116",
      "e54a76bb4d964c51ac2dbc5847f77373",
      "03f717c24f2a4f8d8ce8ed95b7c38cdc",
      "61ceb8b5d8e14793a928c69a011f8de3",
      "949d00c1c98e4882bcc42e277c42fd90",
      "8e9ebd55f154497aa486cd8553d01f9a",
      "9b6ddc6a9a75401b9be4e3493227d0d0",
      "a446740c967f42f89f08d7bb50cc9efc",
      "a259165ab9f94ae08fdb43b5841f8286",
      "ac6810a40a894e049ea82ece1e32070d",
      "d9f992d15c58402ba4f2d4ea36eda158",
      "a823aa26f0284688b085c5c2a14ecd0d",
      "515f5e0866774ccb8c81ac7928f9631c",
      "e0375f2780d34023b472bd27c872792f",
      "6c7d4597e53d4b34b463fda948c8f3f5",
      "84f5acdb89c6498a9537e1db20e56bae",
      "bb5a392dc11d4000b49fe094e549600b",
      "37651fbf4fc14a80a62b915503f36970",
      "da542760cba74e7483a369fc47272a96",
      "d5ebb08a73914ef5ab99f155113988bf",
      "382f274fffe94202bd4e790e6f11e122",
      "daf860a45e7340f5a00f9ca38fac4a33",
      "c96c99c2bef349409ba32d12738d32d5",
      "6fb8eb1364e343bfac6457eaac8317f0",
      "48d7b16d34f34e9e98f72e366fef1102",
      "8b9fcc256d014ae8a703b0e7e294142f",
      "73e86e016f8441d0a9f4f6dc2969aff6",
      "9fcc45272b574f85b74c52acebed085a",
      "83b913c190cb4b6bb95877bdfc65afd9",
      "131b4c8cd6c84783aca0ecd1ca7071c9",
      "5ef95dd5adbb4e4ba78c5fcf5db4b8b0",
      "ef0ab1158b224ff7b58cbc8ac66c5522",
      "f704377514614da4863771e4d457194b",
      "2c58ee11c1ec408d9a5afd4a10fe9cee",
      "2b3a9f46d5a04afa922ed80f00c17bb2",
      "ef0ca581eeb04806ae89eb1a4a4f2fd5",
      "ac551c606a0d44868f5ff2e1fea5f369",
      "cf311a93b0bc4a9f9c488fafa1344a5b",
      "765333e249654a358f8163cbdddda2a3",
      "ab0590931bad4474b1746bdcca071f59",
      "674e2a80961b43338419aec30bf4d06e",
      "8bd9814a5e814d5ab0c2589d770161aa",
      "d6b05587a81b4d3390812047dfaa4a91",
      "83f403de87794aceb283bd0579dc771a",
      "78a13dbeaec64fd4ade52a323a014116",
      "3bd6db4989cc4a1e9c6713c648396d0f",
      "8b2c5a500a6d47ba897b159b05d11ff7",
      "68280b0bc6ff4026b81e560f924dcc9a",
      "b2723b5da5404cf8832de589ecb0aef7",
      "11151564fd5a4854a5f53e0731b11ee4",
      "5d27d300a0e04d54b56276df2295dae9",
      "7640a2cf8fbb4a8c803e32a09a660956",
      "9a3ae702c7b049e1ac0e156435bf2108",
      "5a1b818d52f148aaa42f12583718b080",
      "6f4f2b3851c04f96a0661f737ad26091",
      "8c99d3ce16d846d9872f55351ea40f80",
      "ebd8b379624040efb6860954f54bc0f4",
      "54d367639427485d84fe2b263a44f907",
      "a8b8e6be704549da9f7d88b5de84bc7c",
      "618a41d299524be2a3eb764a66b3dd8b",
      "84290726e35b48e984a4a73e93eb0336",
      "4fec8df1dcc3494ab88b03e3a73f6443",
      "c58a7d589311474697745896d758be89",
      "3c5272213d924fffb593e35e91235d51",
      "ba1131e3523e45b1b7d74ae6faef5f1c",
      "de96c492047042f7882918afe10b6da5",
      "c8f5bf6ebf6a4ebea32ee114ba7b67ec",
      "f0356ae30a4949e18d9266d0d57df4e5",
      "e956a28acb684d7d8f937164ec3e2241",
      "b28ea5371388403dac0c6f98431cc43c",
      "f8013997f6bb4eed8c525f2ab1d1b074",
      "77005384c5684111b93f18f25ad3e895",
      "e87e617e402746bd947c7353e7ff6e11",
      "4407166a2d724a1bb5b2e056d6a1936e",
      "4fb48808bdad4074b212aee50afb75dc",
      "36b38d8a5d6f462db6310ed42770d835",
      "3663dd41c9af4887bd91f2beb7c1a95f",
      "2a02e66e631546d8a57971c600f1fc2a",
      "7e155eb743dc4ff48b88446935467b0c",
      "0160aff4c8ab4dafb27df5d0fa79fafa",
      "9127b1c594ed4ebaa5b5024d9c66b247",
      "19c0193c69a64a999ebd1abeb1df3b7b",
      "7ee5dbb81d1e4f9dbe8a808bbfda234a",
      "cc3e66227d344047b9aebfd0a8c7b2fe",
      "259661bc7e5547d0a000fb8ddc02aff1",
      "289405c55b51438b92aba69b5a979869",
      "03047095f9904360a96fb97c15279f9c",
      "cbca6543fede4dbb8648560eb3eb632b",
      "be84f72fac0e40b7ab22cd5c12c6b480",
      "d3c701cd439045cd92e920bd6e521c5e",
      "3aef6c214b20494ca9ec8924d1c37c4a",
      "339f7caaa3894d0f9bacdf858091febf",
      "ae46874f2a5e43df806bbb2f02d7308e",
      "914bb494d3b74878bcef2c5c2a2ca499",
      "9c8b81168c1746ebab78ca57219c056b",
      "7bef87fd7bf3464981a317bdc3d40c90",
      "46dc27eed3f3414792d720a1e493dae6",
      "be81ad514f7f4b76a209db166ca99e8a",
      "27a32816935c471b940864b55e1a5e8f",
      "673470c337914ae892a0d9596b4a1d5b",
      "c3f7ea4767b048d0bd89ecefa375a9d4",
      "f6d141a47f63433eb1a4dc8d46a65cff",
      "6ffd22c0be8144fca173487394d06cf3",
      "a0d4270ae9c84a3784cd823151c48290",
      "ebecac8dfda8453da9dcf2b0229fe726",
      "7e6047d5121a4727bbd503b4ad18e658",
      "6bbb078e1d2b428d835b87a7fa4acf46",
      "1f24af688eb3445fb86628c195f68d42",
      "5e3895692129463094e11c92b8b4f5a0",
      "395c3849005a45e99b3801da04954b39",
      "e255fc2537c5476fbe092a429d2f2743",
      "c5142e1f6f9e469ab7758b25e47bede6",
      "8c6a632cc1a540198310c3b09798565f",
      "3eee258b1d1342d199d76c8739ab0248"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761860568711,
     "user_tz": -60,
     "elapsed": 3165943,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "fea6a950-ae47-4ce5-a78f-e327a69aa7e0"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.12: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa0509efaada4931b7848ee2d4e6612a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99a38f753bee444d8058d17b223fbaa4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fbda632db4e42e1a8ff489ea1793243"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3abdcf94381422d9dc84e005730cdd4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "558924138f2c44eebea6323ee008772d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcffd9a5313043e2ae9c945c6fb2b798"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4b7ee95063744f6b0ca294de6fc0e31"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949d00c1c98e4882bcc42e277c42fd90"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84f5acdb89c6498a9537e1db20e56bae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73e86e016f8441d0a9f4f6dc2969aff6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf311a93b0bc4a9f9c488fafa1344a5b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2723b5da5404cf8832de589ecb0aef7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "618a41d299524be2a3eb764a66b3dd8b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/363 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8013997f6bb4eed8c525f2ab1d1b074"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19c0193c69a64a999ebd1abeb1df3b7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/363 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae46874f2a5e43df806bbb2f02d7308e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/91 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0d4270ae9c84a3784cd823151c48290"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 363 | Num Epochs = 3 | Total steps = 273\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 16,394,240 of 4,316,473,712 (0.38% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/273 47:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>1.654859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>1.424909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>1.378992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>1.347237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>1.352144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>1.374788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>1.411070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>1.434475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>1.450255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>1.466689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>1.476433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>1.477056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>1.484927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Not an error, but Gemma3ForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=273, training_loss=0.3617619813143552, metrics={'train_runtime': 2941.8537, 'train_samples_per_second': 0.37, 'train_steps_per_second': 0.093, 'total_flos': 2.063889774880416e+16, 'train_loss': 0.3617619813143552, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Salva gli adapter LoRA\n",
    "# model.save_pretrained_merged(\"mistral-base-finetuned-ghigliottinai2\", tokenizer, save_method = \"lora\")\n",
    "# 3. Usa il metodo nativo PEFT 'save_pretrained' sul modello\n",
    "# Questo salva automaticamente solo i pesi LoRA aggiunti.\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/gemma/gemma-4b-ghigliottina-simplerCot-3epoch-tunedByClaude\"\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 4. Salva il Tokenizer separatamente (\u00e8 comunque piccolo)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJqFXuVPE0Sg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761860587770,
     "user_tz": -60,
     "elapsed": 19033,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "040f4ec2-e766-44df-a2c2-b2bbdc651ae6"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/gemma/gemma-4b-ghigliottina-cot-3epoch-tipByClaude/processor_config.json']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Per ricaricare il modello per l'inferenza (in un nuovo notebook)\n",
    "# Ricarica il modello base (in 4-bit)\n",
    "max_seq_length = 2048 # Imposta la lunghezza massima della sequenza\n",
    "dtype = None # Seleziona automaticamente bfloat16 o float16\n",
    "load_in_4bit = True # Usa QLoRA per l'efficienza\n",
    "\n",
    "base_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/gemma-3-4b-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Carica gli adapter LoRA e uniscili al modello base\n",
    "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/gemma/gemma-4b-ghigliottina-cot\")\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Prepara il modello per l'inferenza\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/gemma/gemma-4b-ghigliottina-cot\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPLmyseGzi7r",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761823249530,
     "user_tz": -60,
     "elapsed": 81129,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "05278895965856271627"
     }
    },
    "outputId": "7d35b9d7-e934-4f99-a236-eb081490abd7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.10.11: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = create_prompt(clues=['maestro', 'fuori', 'cicale', 'rispondere', 'proteste'])\n",
    "# Tokenizzazione - Explicitly pass None for other possible multimodal inputs\n",
    "inputs = tokenizer(\n",
    "    text=[prompt], # Pass the input_text in a list and explicitly label it as text\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Genera la risposta\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
    "\n",
    "# Stampa l'output\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWnpch__XYfw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761860784774,
     "user_tz": -60,
     "elapsed": 142643,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "13047890983418185086"
     }
    },
    "outputId": "3db57a77-e05a-4a39-e601-77a535904b1f"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bos>Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole\n",
      " LE CINQUE PAROLE: MAESTRO, FUORI, CICALE, RISPONDERE, PROTESTE\n",
      " RICERCA DI SOLUZIONI:\n",
      " LE CINQUE PAROLE SONO: \"MAESTRO\", \"FUORI\", \"CICALE\", \"RISPONDERE\", \"PROTESTE\"\n",
      " La prima parola \u00e8 MAESTRO. La prossima \u00e8 la parola che si lega al contesto di MAESTRO. La prossima \u00e8 la parola \"CICALE\"\n",
      "\n",
      "RAGIONAMENTO:\n",
      "1. LA PRIMA SOLUZIONE RAGGIUNTA \u00c8: Il MAESTRO DEVE UNIRE PAROLE COME [ES. ASTRONOMI].\n",
      "MAESTRO [GRUPPO ASTRONOMI] :\n",
      "   MAESTRO [GRUPPO ASTRONOMI] NON ESCE.\n",
      "2. LA SECONDA SOLUZIONE RAGGIUNTA \u00c8: GRUPPO [ES. TRAMER] NON ESCE.\n",
      "FUORI [GRUPPO TRAMER]: [ES. GRANDE TRAMER]\n",
      "3. L'ESERCIZIO DEVE SOGLIARE [GRUPPO ESPLOSIVO].\n",
      "4. SE [ESPLOSIVO GRUPPO] NON ESCE:\n",
      "LE SEGUENTI SONO [GRUPPO ESPOSIVO].\n",
      "\n",
      "RAGIONAMENTO DEGLI ESSERI [ESPERIMO]:\n",
      "L'ESPLOSIVO GRUPPO DEVE SOGLIARE [ESPLOSIVO GRUPPO].\n",
      "ESPLOSIVO GRUPPO DEVE [GRUPPO ESPOSIVO].\n",
      "\n",
      "[GRUPPO ESPLOSIVO] DEVE CONCIDERARE [GRUPPO ESPLOSIVO].\n",
      "\n",
      "MAESTRO [GRUPPO ASTRONOMI]:\n",
      "MAESTRO [GRUPPO ASTRONOMI] DEVE [GRUPPO ESPLOSIVO].\n",
      "PROBABILE GRUPPO ASTRONOMI: [ES. [GRUPPO ASTRONOMI].].\n",
      "GRUPPO ASTRONOMI RAGGIUNTO: [ES. ESPLOSIVO GRUPPO].\n",
      "\n",
      "GRANDE ESPLOSIVO [GRUPPO ESPLOSIVO].\n",
      "\n",
      "GRANDE [GRUPPO ESPLOSIVO]: [GRUPPO ASTRONOMI]\n",
      "\n",
      "GRANDE [GRUPPO ASTRONOMI]:\n",
      "GRANDE [GRUPPO ASTRONOMI] NON DEVE [GRUPPO ASTRONOMI].\n",
      "GRANDE ESPLOSIVO [\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# Per ricaricare il modello per l'inferenza (in un nuovo notebook)\n",
    "# Ricarica il modello base (in 4-bit)\n",
    "# base_model, _ = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_name,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     load_in_4bit = True,\n",
    "# )\n",
    "\n",
    "# # Carica gli adapter LoRA e uniscili al modello base\n",
    "# peft_model = PeftModel.from_pretrained(base_model, \"mistral-base-finetuned-ghigliottinai\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Prepara il modello per l'inferenza\n",
    "\n",
    "prompt = create_prompt(clues=['maestro', 'fuori', 'cicale', 'rispondere', 'proteste'])\n",
    "print(prompt)\n",
    "# Tokenizzazione\n",
    "inputs = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generazione del testo (completamento)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer = streamer,\n",
    "    max_new_tokens = 256,\n",
    "    use_cache = True,\n",
    "    #stopping_criteria = stopping_criteria\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761666869319,
     "user_tz": -60,
     "elapsed": 50596,
     "user": {
      "displayName": "Antonio Gagliostro",
      "userId": "08652005224032424281"
     }
    },
    "outputId": "fd5b107c-6fc8-4909-e79f-f6e709312da3",
    "id": "AitU70ZIhjK_"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pensa passo dopo passo usando la chain of thoughts \n",
      "ISTRUZIONE: Trova la soluzione al seguente gioco: date in input 5 parole non legate tra loro, dimmi qual \u00e8 la parola in comune che risulta essere legata ad ognuna delle 5 parole\n",
      " LE CINQUE PAROLE: MAESTRO, FUORI, CICALE, RISPONDERE, PROTESTE\n",
      "\n",
      " SOLUZIONE: MAESTRO = CICALE ( entrambi hanno la lettere MA)\n",
      " SOLUZIONE: FUORI = PROTESTE ( entrambi hanno la lettera O)\n",
      " SOLUZIONE: CICALE = RISPONDERE ( entrambi hanno la lettere CE)\n",
      " SOLUZIONE: RISPONDERE = MAESTRO\n",
      " SOLUZIONE: CICALE = PROTESTE\n",
      " SOLUZIONED: MAESTRO = RISPONDERE\n",
      " SOLUZIONED: CICALE = PROTESTE\n",
      " SOLUZIONED: RISPONDERE = MAESTRO\n",
      " SOLUZIONED: CICALE = PROTESTE\n",
      " SOLUZIONED: PROTESTE = RISPONDERE\n",
      " SOLUZIONED: MAESTRO = CICALE\n",
      " SOLUZIONED: MAESTRO = RISPONDERE\n",
      " SOLUZIONED: CICALE = PRISTO\n",
      " SOLUZIONED: PRISTO = RISPONDERE\n",
      " SOLUZIONED: RISPONDERE = MAESTRO\n",
      " SOLUZIONED: PRISTO =  MAESTRO\n",
      " SOLUZIONED:  MAESTRO = RISPONDERE\n",
      " SOLUZIONED:  PRISTO\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ho aggiunto l'uso esplicito della CoT in gemma-3 4B, vediamo come si comporta con la sola CoT.\n",
    "In un secondo momento potrei provare a finetunare partendo da Gemma con reasoning sul dataset creato da me e vediamo come evolve"
   ],
   "metadata": {
    "id": "jNjl_lR1GYRj"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "XHWOKHi6GV_1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Velvet"
   ],
   "metadata": {
    "id": "8zhS1dobt253"
   }
  }
 ]
}